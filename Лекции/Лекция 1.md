# Введение. Асимптотический анализ (О-нотация). Сортировки
## Асимптотический анализ
При оценке эффективности работы есть две характеристики: **время** и **память**.

**RAM-модель** (Random Access Machine) - это такая вычислительная машина, которая обладает следующими свойствами:
1) **Однопроцессорная единица**. Все инструкции выполняются последовательно, одна за другой.
2) **Бесконечная память** с произвольным доступом. Каждая ячейка памяти имеет уникальный адрес и время доступа к любой ячейке. **Время доступа константное** (не зависит ни от адреса, ни от размера данных). 
3) **Базовые операции**. (`+, -, /, %, ==, >, <, =`, вызов функции, доступ к элементу) - O(1)


Рассмотрим простейшую задачу для нахождения минимального числа в массиве. 
```python
min_val = arr[0] # O(1) - присваивание
for num in arr[1:]: # n - 1 итераций
	if num < min_val # O(1) - присваивание
		min = num # O(1) - присваивание
print(min_val) # O(1)
```
Посчитаем асимптотическую сложность данного кода. 

$$
T(n) = 1 + 2(n-1) + 1 = 2n - 2 + 2 = O(n)
$$

Константы можно опускать, в целом временная сложность O(n) = n

В такой нотации опускаются все константы, остаются только переменные. 

### O-нотация
Говорят, что f(n) = O(g(n)) если существуют такие положительные константы C и Eo больше N, то для любого n > n0 следует что f(n) <= c * g(n) ==дописать математической нотацией==

В таком случае функция f(n) растет не быстрее g(n) с точностью до постоянного множителя n для всех достаточно больших n, а функция g(n) в таком случае называется **верхней асимптотической оценкой**. 

Пример: f(n) = 5n + 2 = O(n)
Докажем это.
5n + 2 <= c * g(n)
5n + 2 <= c * n
c = 6
5n + 2 <= 6n
n >= 2

### Другие асимптотические нотации
1) **Омега-нотация** (Ω) - асимптотически нижняя нотация. Аналогично О-нотации, но только c * g(n) <= f(n). ==дописать==
2) **Нотация Тэта** (Θ) - асимптотически точная оценка. Θ(g(n)) существует с1 > 0, c2 > o, существует n0 < N: all n >= n0. f(n) = Θ(g(n)). 

O(...) работает "не дольше, чем"
Ω(...) работает "не быстрее чем"
Θ(...) работает "примерно как"

---
## Сортировки
**Суть**: Массив мысленно делится на две части: отсортированную и неотсортированную. 
Изначально отсортированная часть содержит 1 элемент. Затем берем по одному элементу из неотсортированной части и вставляем на определенное место в отсортированной части.

```python title:"Псевдокод"
# len(arr) = n
for i in range(1, len(arr)): # n - 1
	key = arr[i] # O(1)
	j = i - 1 # O(1)
	while j >= 0 and arr[j] > key: # i шт
		arr[j + 1] = arr[j]; # O(1)
	arr[j + 1] = key # O(1)
```
$$
T(n) = (n - 1)(3n - 3 + n) = (n - 1)(4n - 3) = O(n^2)
$$
==доказать математически==

$O(sqrt(n))$ - асимптотическая сложность задачи для проверки простоты числа.

Пример:
```python
def f(n):
	if n = 1:
		return 1
	return f(n - 1) + 1
```
Асимтотическая сложность:
$$f(n) -> f(n - 1) -> f(n - 2) -> ... -> F(1) - O(n)$$
Изменим одну строчку: ==почему (см формулу разбиение логарифма)==
```python
...
	return f(n // 2) + 1 # O(logn)
...
```

$$f(n) -> f(n / 2) -> f(n / 4) -> ... -> f(1) - O(logn)$$

---
### Сортировка слиянием

```python
def merge_sort(arr):
	if(arr) <= 1: 
		return arr
	min = len(arr) // 2
	left_half = arr[:mid]
	right_half = arr[mid:]
	
	left_sorted = merge_sort(left_half)
	right_sorted = merge_sort(right_half)
	
	return merge(left_sorted, right_sorted)
	

def merge(left, right):
	res = []
	i = j = 0
	
	while i < len(left) and j < len(right):
		if left[i] <= right[j]:
			res.append(left[i])
			i += 1
		else:
			res.append(right[j])
			j += 1
	res.extend(left[i:])
	res.extend(right[j:])
	return res	
```

a: 1, 2, 3, 5, 7
b: 1, 3, 7, 8, 9
c: 1, 1, 2 ...

Предпочтительнее использовать For а не While:
while(...) {} <=> for (; ... ;)

Оценка сложности: ==доказать, ДЗ==

$T(n) = O(1), n <= 1$
$T(n) = 2*T(n/2) + O(n)$ - время на рекурсивную сортировку двух половин + время необходимое для слияния двух массивов (Первое + второе слагаемое)

Нарисовать дерево (O(n) -> O(n/2) ...) } log^k

На каком-то уровне всего будет два элемента

Итого сложность будет $O(n/2^k)*2^k$
Итоговая оценка будет 

## Мастер-теорема
$$T(n) = a * T(n/b) + f(n), где a>=1, b>1$$
$T(n)$ - общее время работы алгоритма на входе от n
общее время работа работы алгоритма

$а$ - количество подзадач (рекурсивных вызовов), на которые делится исходная задача

$T(n/b)$ - описывает время на решение подзадач

$n/b$ размер каждой подзадачи

$T(n/b)$ - время на решение каждой подзадачи

$f(n)$ - время на выполнение задачи, не имеющих рекурсивных подзадач

1. $f(n) = T(n) = Θ(n^(log_b(a-e))$ где $e$ > 0, то тогда $T(n) = Θ(n^(log_b(a))$ 
2. $f(n) = Θ(n^(log_b(a)))$ $T(n) = Θ(n^(log_b(a))*logn)$
3. $f(n) = Ω(n^(log_b(a+e)))$ то $a*f(n/b) <= c*f(n) T(n) = Θ(f(n))$

$T(n) = 2 * T(n/2) + Θ(n)$ - Merge sort в виде мастер-теоремы

## Быстрое умножение полиномов
Обычно скорость умножения полиномов это $n*m$ где n, m - старшие степени полиномов. 

$A(x) = A_(high)(x)*x^n/2+A_low(x)$
$B(x) = B_(high)(x)*x^n/2+B_low(x)$

Найдем произведение в лоб. 

$A*B = (A_high*B_high) * x^n + (A_high*B_low + A*low*B_high)*x^n/2 + A_low*B_low$

**Алгоритм Карацуба** -  быстрое умножение полиномов
Алгоритм Краца - быстрое умножение матриц

Карацуба придумал, как можно обойтись тремя умножениями вместо четырех. 

$D_0 = A_high*B_high$
$D_1=(A_{low }+ A_{high})*(B_{low}+B_{high})$
$D_2=(A_{high}*B_{high})$

Тогда можно переписать ту же формулу в другом виде.
$$A*B = D_0 * x^n + (D_1 - D_0 - D_2)*x^n/2 + D_2$$


По мастер-теореме получаем следующую сложность:
$$T(n) = 3*T(n/2)+O(n)$$
$$T(n) = O(n^{log_23})$$

